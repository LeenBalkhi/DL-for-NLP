{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310f24e6a7b88cce",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba8a222d9df960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import re\n",
    "import glob\n",
    "import collections\n",
    "import string\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74378dc680068d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorter(item):\n",
    "    \"\"\" Function tha gets only the first number of the name of the file and organizes the files base on that\"\"\"\n",
    "    \n",
    "    return int(os.path.basename(item).split('_')[0])\n",
    "\n",
    "def read_raw_text(path_data):\n",
    "    \"\"\" Function for reading the raw data in the .txt files. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_data: str\n",
    "        path of the folder that contains the data that is going to be used. (should be test or train)\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    data,scores: array_like\n",
    "        Data arrays, X is an array of shape [#documents of the dataset, #words in the vocabulary], y is an array of shape [#documents,] \n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    scores = []\n",
    "    \n",
    "    sentiments = ['pos', 'neg']\n",
    "    for sentiment in sentiments:\n",
    "        path_vocab_pos = os.path.join(\".\", \"aclImdb\", path_data, sentiment, \"*.txt\")\n",
    "        \n",
    "        for filename in sorted(glob.glob(path_vocab_pos), key=sorter):\n",
    "            \n",
    "            with open(filename, encoding='utf8') as f:\n",
    "                \n",
    "                lines = f.read()\n",
    "                \n",
    "                data.append(lines)\n",
    "                scores.append(int(os.path.basename(filename).split('_')[1].strip('.txt')))\n",
    "    return data, scores\n",
    "\n",
    "\n",
    "def read_vocab():\n",
    "    \"\"\" Function for reading the vocabulary (.initial_vocab file). \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    initial_vocab: list\n",
    "        list with the values different tokens that compose the vocabulary ...... \n",
    "    \"\"\"\n",
    "    \n",
    "    path_vocab = os.path.join(\".\", \"aclImdb\", \"imdb.initial_vocab\")\n",
    "    \n",
    "    with open(path_vocab, encoding='utf-8') as f:\n",
    "        lines = f.read()\n",
    "\n",
    "    lines = lines.split('\\n')\n",
    "    \n",
    "    vocab = []\n",
    "    for line in lines:\n",
    "        vocab.append(line)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefbf6c55ef2c7d",
   "metadata": {},
   "source": [
    "## Task 1: Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f840b8a322805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy your pre-processing pipeline code from the previous exercise here\n",
    "def pre_process(\n",
    "    reviews,\n",
    "    tokenize_punct=False,\n",
    "    lowercase=False,\n",
    "    remove_punct=False,\n",
    "    remove_high_freq_terms=False,\n",
    "    high_freq_threshold=0.5,\n",
    "    replace_numbers=False,\n",
    "    remove_stopwords=False\n",
    "):\n",
    "    if tokenize_punct:\n",
    "        tokens = []\n",
    "        for review in reviews:\n",
    "            words = re.split(r\"[^a-zA-Z0-9'-]+\", review)        \n",
    "            tokens.append(words)\n",
    "    \n",
    "    if lowercase:\n",
    "        tokens = [[word.lower() for word in docu] for docu in tokens]\n",
    "\n",
    "    if remove_punct:\n",
    "        tokens = [[item.translate(str.maketrans('', '', string.punctuation)) for item in words] for words in tokens]\n",
    "\n",
    "    if remove_high_freq_terms:\n",
    "        word_counts = {}\n",
    "        total_words = 0\n",
    "\n",
    "        for docu in tokens:\n",
    "            for word in docu:\n",
    "                total_words += 1\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        num_top_words = int(len(sorted_words) * high_freq_threshold)\n",
    "        high_freq_words = set(word for word, count in sorted_words[:num_top_words])\n",
    "\n",
    "        tokens = [[word for word in docu if word not in high_freq_words]for docu in tokens]\n",
    "\n",
    "    if replace_numbers:\n",
    "        tokens = [[re.sub(r'\\d', \"<NUM>\", word) for word in words if \"_\" not in word] for words in tokens]\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        tokens = [[word for word in words if word.lower() not in stop_words] for words in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d00e2edb7825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the raw data\n",
    "data, scores = read_raw_text('train')\n",
    "data_test, scores_test = read_raw_text('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b6df8eb909544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pre-processing pipeline\n",
    "pre_processed_data = pre_process(data, lowercase=True, remove_punct=True)\n",
    "pre_processed_data_test = pre_process(data_test, lowercase=True, remove_punct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9e3a91b0ca578",
   "metadata": {},
   "source": [
    "## Task 2: Byte-Pair Tokenizer\n",
    "\n",
    "You can refer to this [Hugging Face tutorial](https://huggingface.co/learn/llm-course/en/chapter6/5) for a detailed explanation of the BPE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec91f5322befbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, base_vocab: str, num_merges: int = 1000):\n",
    "        self.base_vocab = base_vocab\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = None\n",
    "        \n",
    "#corpus needs to have the frequency of each word\n",
    "#to train i should tokenize on characters, and start populating with character-level tokens + freq of each char\n",
    "# then iterate over inital vocab to calculate the freq of adj chars\n",
    "#iteration should be within the specified range num_merges\n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        pass # todo\n",
    "\n",
    "    def tokenize(self, text: str) -> List[List[str]]:\n",
    "        pass # todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b962dead76165",
   "metadata": {},
   "source": [
    "### 2 (a): Base Vocabulary = Characters from Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098e2f03e8756ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique characters from the data\n",
    "unique_chars = # todo\n",
    "\n",
    "# Train the tokenizer\n",
    "bpe_char_based = BPETokenizer(base_vocab=unique_chars, num_merges=1000)\n",
    "bpe_char_based.train(pre_processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f703149dc6f6212",
   "metadata": {},
   "source": [
    "### 2 (b): Base Vocabulary = ASCII Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f3df97629f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_chars = list(string.printable)\n",
    "\n",
    "bpe_ascii_based = BPETokenizer(base_vocab=ascii_chars, num_merges=1000)\n",
    "bpe_ascii_based.train(pre_processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040785368d1f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the two vocabularies\n",
    "print(\"Char-based initial_vocab sample:\")\n",
    "print(list(bpe_char_based.vocab.items())[:10])\n",
    "\n",
    "print(\"ASCII-based initial_vocab sample:\")\n",
    "print(list(bpe_ascii_based.vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0accb841574290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Tokenization of a New/Unknown Word\n",
    "unknown_word = \"backpropagationlessness\"\n",
    "\n",
    "print(\"Char-based tokenization:\", bpe_char_based.tokenize_word(unknown_word))\n",
    "print(\"ASCII-based tokenization:\", bpe_ascii_based.tokenize_word(unknown_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6a186147a1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more comparison ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf732a0f4ab46",
   "metadata": {},
   "source": [
    "## Task 3: WordPiece Tokenizer\n",
    "\n",
    "You can refer to this [Hugging Face tutorial](https://huggingface.co/learn/llm-course/en/chapter6/6) for a detailed explanation of the WordPiece algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623cc3770053da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab_size=1000, initial_vocab=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.initial_vocab = set(initial_vocab) if initial_vocab else set()\n",
    "        self.vocab = {}\n",
    "        \n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        pass # todo\n",
    "\n",
    "    def tokenize(self, text: str) -> List[List[str]]:\n",
    "        pass # todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be380a10518ee01a",
   "metadata": {},
   "source": [
    "### 3 (a): Base Vocabulary = Characters from Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d7d6c2b72b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chars = # todo\n",
    "tokenizer_a = WordPieceTokenizer(vocab_size=1000, initial_vocab=corpus_chars)\n",
    "vocab_a = tokenizer_a.train(pre_processed_data)\n",
    "print(f\"Vocabulary A (corpus chars): {sorted(vocab_a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a2c84bd061f15",
   "metadata": {},
   "source": [
    "### 3 (b): Base Vocabulary = Characters from Reviews + ASCII Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb4ac09dba82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_chars = set(string.printable)\n",
    "initial_vocab = sorted(corpus_chars.union(ascii_chars))\n",
    "tokenizer_b = WordPieceTokenizer(vocab_size=1000, initial_vocab=initial_vocab)\n",
    "vocab_b = tokenizer_b.train(pre_processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ae659f632c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Tokenization of a New/Unknown Word\n",
    "unknown_word = \"backpropagationlessness\"\n",
    "print(\"WordPiece A tokenization:\", tokenizer_a.tokenize_word(unknown_word))\n",
    "print(\"WordPiece B tokenization:\", tokenizer_b.tokenize_word(unknown_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c4e4d14435b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more comparison ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33816e87ce958841",
   "metadata": {},
   "source": [
    "## Task 4: Hugging Face Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163964ec653e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Byte-Pair Encoder (BPE)\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790713337f1a1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face WordPiece Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5887194c04445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the different tokenizers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
