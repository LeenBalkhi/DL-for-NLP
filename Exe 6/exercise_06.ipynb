{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911c3ae1c5c87a5b",
   "metadata": {
    "id": "911c3ae1c5c87a5b"
   },
   "source": [
    "# Exercise 6: Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64b714f9b2c00c",
   "metadata": {
    "id": "8f64b714f9b2c00c"
   },
   "source": [
    "## Task 1: Implementation of Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f834b",
   "metadata": {
    "id": "b31f834b"
   },
   "source": [
    "padding is done at the end in transformer... why not at the begining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1750868015706,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.downloader import load as gensim_load\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9bd249ac0f512cd",
   "metadata": {
    "executionInfo": {
     "elapsed": 35642,
     "status": "ok",
     "timestamp": 1750868051335,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "b9bd249ac0f512cd"
   },
   "outputs": [],
   "source": [
    "glove = gensim_load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "307022694dda76ac",
   "metadata": {
    "executionInfo": {
     "elapsed": 1526,
     "status": "ok",
     "timestamp": 1750868052876,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "307022694dda76ac"
   },
   "outputs": [],
   "source": [
    "def load_imdb(n_samples=100):\n",
    "    dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "    # How many samples per class\n",
    "    n_per_class = n_samples // 2\n",
    "\n",
    "    # Filter each class\n",
    "    pos = dataset.filter(lambda x: x[\"label\"] == 1).shuffle(seed=42).select(range(n_per_class))\n",
    "    neg = dataset.filter(lambda x: x[\"label\"] == 0).shuffle(seed=42).select(range(n_per_class))\n",
    "\n",
    "    # Combine and shuffle\n",
    "    balanced = concatenate_datasets([pos, neg]).shuffle(seed=42)\n",
    "\n",
    "    texts = balanced[\"text\"]\n",
    "    labels = balanced[\"label\"]\n",
    "    return texts, labels\n",
    "\n",
    "texts, labels = load_imdb(n_samples=10000) # adjust n_samples based on your computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161194cf4545ec7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1750869906827,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "8161194cf4545ec7",
    "outputId": "aba4e7b6-2843-432b-cb87-79ad2bf8abb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     c:\\Users\\balkh\\miniconda3\\envs\\myenv\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt_tab')\n",
    "\n",
    "def vectorize(tokens, max_len=100):\n",
    "    tokens = word_tokenize(tokens.lower())\n",
    "\n",
    "    #embedded represnetation of input tokens\n",
    "    X =[]\n",
    "\n",
    "    for token in tokens [:max_len]:\n",
    "        if token in glove:\n",
    "            X.append(glove[token])\n",
    "        else:\n",
    "            X.append(np.zeros(glove.vector_size))\n",
    "    while len(X) < max_len:\n",
    "        X.append(np.zeros(glove.vector_size))\n",
    "\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b19bc049f502c5",
   "metadata": {
    "executionInfo": {
     "elapsed": 3566,
     "status": "ok",
     "timestamp": 1750869912161,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "27b19bc049f502c5"
   },
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "X = torch.tensor([vectorize(t) for t in texts], dtype=torch.float32)\n",
    "Y = torch.tensor(labels, dtype=torch.long) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef678097e15837ba",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1750869914525,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "ef678097e15837ba"
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        # define the weight matrices W_Q, W_K, and W_V\n",
    "        self.model = d_model\n",
    "        self.d_k = d_k\n",
    "\n",
    "        # learnable weight matrices\n",
    "        self.W_Q = nn.Linear(d_model, d_k) #x @ weight + bias\n",
    "        self.W_K = nn.Linear(d_model, d_k) #x @ weight + bias\n",
    "        self.W_V = nn.Linear(d_model, d_k) #....\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #vector production\n",
    "        W_Q = self.W_Q(x)\n",
    "        W_K = self.W_K(x)\n",
    "        W_V = self.W_V(x)\n",
    "\n",
    "        #attention score using dot product between queries and keys\n",
    "        scores = torch.matmul (W_Q, W_K.transpose(-2,-1)) / np.sqrt(self.d_k)\n",
    "\n",
    "        W_attention = F.softmax(scores, dim=-1)\n",
    "        #attention-weighted sum of the value vector\n",
    "        out = torch.matmul(W_attention, W_V)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c53e5b0a61c1e",
   "metadata": {
    "id": "42c53e5b0a61c1e"
   },
   "source": [
    "## Task 2: Adding a Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b5594f0e9dd5d",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1750869916072,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "b59b5594f0e9dd5d"
   },
   "outputs": [],
   "source": [
    "class BinaryClassificationModel(nn.Module):\n",
    "    def __init__(self, d_model, d_k, temp= 2, dim = 128):\n",
    "        super().__init__()\n",
    "        # attention layer\n",
    "        self.attention =SelfAttention(d_model, d_k)\n",
    "\n",
    "        #multilayer perceptron\n",
    "        #linear then non-linear, then linear.\n",
    "        #project input into a higher dimensional feature space (to learn more compelex combinatuion of features)\n",
    "\n",
    "        #then Relu (non-linear) to add non-0linear transformations for classification\n",
    "\n",
    "        #then linear to compress features into one logit for binarz classification\n",
    "\n",
    "\n",
    "        self.layers = nn.Sequential (\n",
    "            nn.Linear(d_k, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, 2)\n",
    "        )\n",
    "        self.temperature = temp\n",
    "        # add temperature to softmax\n",
    "\n",
    "\n",
    "#before going into the MLP, the attention matrix will need ro be multiplied with what i assume is X. where isthe bias here? is it already included in the layer variable? considering it's sequential nn?\n",
    "\n",
    "    def forward(self, x):\n",
    "        atten_out= self.attention(x)\n",
    "\n",
    "        #to summerize the sequence of tokens rather than looking at tokens (one prediction oer input sequence)\n",
    "        pooled = atten_out.mean(dim=1)\n",
    "        logits = self.layers(pooled)\n",
    "\n",
    "\n",
    "        #confused??? should i really be using softmax for binary classification. but i need to be able to modify the temperature\n",
    "        #probs = F.softmax(logits / self.temperature, dim=1)\n",
    "        #solved. divide logits by temp before passing to CELoss\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b59c6e05c7cb0a",
   "metadata": {
    "id": "f0b59c6e05c7cb0a"
   },
   "source": [
    "## Task 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a0bc8fb29053a",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1750869918088,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "e48a0bc8fb29053a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "def train_model(model, X, Y, epochs=10, lr=1e-3, batch_size=8):\n",
    "    model.train()\n",
    "\n",
    "    #should i be using BCE becaue it's binary classification task? i don't think it'll work with sigmoid.. it didn't, model got stuck and didn't learn. predictions were stuck at 0.7310586 = Sigmoid(1)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss=0\n",
    "        for x, y in loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(x)\n",
    "\n",
    "            # modifying the logits directy before passing to CEL as it already uses a softmax\n",
    "            scaled_logits = logits / model.temperature\n",
    "            loss = criterion(scaled_logits, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss+= loss.item() * x.size(0)\n",
    "        avg_loss = total_loss/ len(loader.dataset)\n",
    "        print(f\"epoch {epoch+1}, loss: {avg_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ceebf114dda52d05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8355,
     "status": "ok",
     "timestamp": 1750870502197,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "ceebf114dda52d05",
    "outputId": "4dedc11c-a1be-4a13-bf9f-ed3b2d4b9196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.553\n",
      "epoch 2, loss: 0.500\n",
      "epoch 3, loss: 0.483\n",
      "epoch 4, loss: 0.475\n",
      "epoch 5, loss: 0.466\n",
      "epoch 6, loss: 0.462\n",
      "epoch 7, loss: 0.454\n",
      "epoch 8, loss: 0.448\n",
      "epoch 9, loss: 0.444\n",
      "epoch 10, loss: 0.440\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "d_model = X.shape[-1] # determine d_model (e.g., based on X)\n",
    "## based on input dimension of X. basically 100, because we're using glove-100\n",
    "d_k = 64 # try different values for d_k\n",
    "model = BinaryClassificationModel(d_model=d_model, temp=6, d_k=d_k)\n",
    "train_model(model, X, Y)\n",
    "\n",
    "def evaluate_model(model, X, Y, temperature=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "    preds_np = preds.numpy()\n",
    "    labels_np = Y.numpy()\n",
    "\n",
    "    acc = accuracy_score(labels_np, preds_np)\n",
    "    rec = recall_score(labels_np, preds_np)\n",
    "    f1 = f1_score(labels_np, preds_np)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Recall:   {rec:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "#evaluate_model(model, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b553763845ec",
   "metadata": {
    "id": "717b553763845ec"
   },
   "source": [
    "## Task 4: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed06b756f3828381",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1750870599996,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "ed06b756f3828381"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    model.eval()\n",
    "    vectors = vectorize(text)\n",
    "    x = torch.tensor([vectors], dtype=torch.float32) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "    sentiment = \"positive\" if pred == 1 else \"negative\"\n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10aff316ffbf98cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1750870506007,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "10aff316ffbf98cc",
    "outputId": "6019d99c-74f3-45df-cfd6-26b4ac9d5ff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This movie was fantastic and full of suspense!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb0346227e20cadf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1750870661894,
     "user": {
      "displayName": "Leen Balkhi",
      "userId": "12555943827126108450"
     },
     "user_tz": -120
    },
    "id": "bb0346227e20cadf",
    "outputId": "04d7732e-08bc-4b02-b83d-1f24e7f21aa3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This movie was boring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "acce2aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = (\n",
    "    \"While the film certainly had its moments of brilliance, especially in its cinematography and musical score, \"\n",
    "    \"it was ultimately bogged down by a convoluted plot and inconsistent character development. \"\n",
    "    \"The lead actor gave a compelling performance, yet the supporting cast felt miscast and underused. \"\n",
    "    \"By the end, I wasn’t sure whether I was moved or just confused. It’s a film that aims high, \"\n",
    "    \"but its ambition may have outpaced its execution.\"\n",
    ")\n",
    "\n",
    "predict_sentiment(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94e0c9",
   "metadata": {},
   "source": [
    "Continuing with the analogy proposed in the lecture with a search engine, think of a particular engine, the engine of your preferences. What would be the keys (K), the queries (Q) and the values (V )?\n",
    "Query would be what you type.\n",
    "value the info from the webpage\n",
    "\n",
    "sqaure root for the dimension in softmax is basically a scale to control the values. \n",
    "\n",
    "(K is usually very big. the multiplication will only make it bigger). \n",
    "\n",
    "if you scale by dk you will have a more stable softmax between 1 and 0\n",
    "\n",
    " we can compare with csine simimlary as we eventually end up witha  similary matrix between the keys and queries\n",
    "\n",
    "\n",
    "during inference, the results are more free and you would have \"hallucinations\".. it's not tuning. when tuning you need to train the model after you tweak the parameters. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Pyenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
