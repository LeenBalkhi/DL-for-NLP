{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1931bc758e933f2e",
   "metadata": {},
   "source": [
    "# Exercise 5: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722560fbe3cd6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorter(item):\n",
    "    \"\"\" Function tha gets only the first number of the name of the file and organizes the files base on that\"\"\"\n",
    "    \n",
    "    return int(os.path.basename(item).split('_')[0])\n",
    "\n",
    "def read_raw_text(path_data):\n",
    "    \"\"\" \n",
    "    Function for reading the raw data in the .txt files. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_data: str\n",
    "        path of the folder that contains the data that is going to be used. (should be test or train)\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    data,scores: array_like\n",
    "        Data arrays, X is an array of shape [#documents of the dataset, #words in the vocabulary], y is an array of shape [#documents,] \n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    scores = []\n",
    "    \n",
    "    sentiments = ['pos', 'neg']\n",
    "    for sentiment in sentiments:\n",
    "        path_vocab_pos = os.path.join(\".\", \"aclImdb\", path_data, sentiment, \"*.txt\")\n",
    "        \n",
    "        for filename in sorted(glob.glob(path_vocab_pos), key=sorter):\n",
    "            \n",
    "            with open(filename, encoding='utf8') as f:\n",
    "                \n",
    "                lines = f.read()\n",
    "                \n",
    "                data.append(lines)\n",
    "                scores.append(int(os.path.basename(filename).split('_')[1].strip('.txt')))\n",
    "    return data, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a767d206fb9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "corpus, scores = read_raw_text('train')\n",
    "corpus_test, scores_test = read_raw_text('test')\n",
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aaaa27849594a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def pre_process(\n",
    "    reviews,\n",
    "    tokenize_punct=False,\n",
    "    lowercase=False,\n",
    "    remove_punct=False,\n",
    "    remove_high_freq_terms=False,\n",
    "    high_freq_threshold=0.5,\n",
    "    replace_numbers=False\n",
    "):\n",
    "    # todo\n",
    "    \n",
    "    return tokenized_reviews\n",
    "\n",
    "tokenized_corpus = pre_process(corpus, tokenize_punct=True, lowercase=True, remove_punct=True)\n",
    "tokenized_corpus_test = pre_process(corpus_test, tokenize_punct=True, lowercase=True, remove_punct=True)\n",
    "labels = [bool(score > 5)  for score in scores]\n",
    "labels_test = [bool(score > 5) for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba756e7882f51085",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_corpus = 10000 # adjust the number of samples you want to use depending on your memory capacity\n",
    "\n",
    "def stratified_sample(data, labels, n):\n",
    "    data_sampled, _, labels_sampled, _ = train_test_split(\n",
    "        data, labels, train_size=n, stratify=labels, random_state=42\n",
    "    )\n",
    "    return data_sampled, labels_sampled\n",
    "\n",
    "# Apply to training and test sets\n",
    "tokenized_corpus, labels = stratified_sample(tokenized_corpus, labels, n_corpus)\n",
    "tokenized_corpus_test, labels_test = stratified_sample(tokenized_corpus_test, labels_test, n_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6deaeb8b04d84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(labels), Counter(labels_test) # verify that the sampling is stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64dab92f718602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        # define the layers of the RNN model here (don't use the nn.rnn layer)\n",
    "        # todo\n",
    "\n",
    "        ###bias is in implied when defining thelayer. bias ?\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc71b4bd1c4f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vocabulary\n",
    "vocab = set(token for review in tokenized_corpus for token in review)\n",
    "vocab = {token: idx for idx, token in enumerate(vocab, start=2)}\n",
    "PAD_TOKEN = \"<PAD>\" # padding is usually done at the end because in the begining \n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN_ID = 0\n",
    "UNK_TOKEN_ID = 1\n",
    "vocab[PAD_TOKEN] = PAD_TOKEN_ID\n",
    "vocab[UNK_TOKEN] = UNK_TOKEN_ID\n",
    "\n",
    "idx_2_vocab = {idx: token for token, idx in vocab.items()}  # invert the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7af377406349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a04e938803af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc502503f27db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033ccaf74facec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vary the hyper parameters (pad_len, embedding_dim, hidden_dim) and analyze their influence\n",
    "# What happens if the padding length is too short or too long?\n",
    "# How does the computational complexity of the RNN model scale with the padding length, embedding dimension, and hidden dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bddc83",
   "metadata": {},
   "source": [
    "theo questions\n",
    "\n",
    "\n",
    "1- diff between vanilla and lstm. \n",
    "lstm has gates, more complex than vanilla rnn\n",
    "\n",
    "vanishing gradient = goes down to zero.\n",
    "lstm gates need to be properly trained. it helps but desn't solve it\n",
    "\n",
    "lstm gets better with long term dep, \n",
    "\n",
    "2- computational complexity. .. its a sequential model. state i needs info from state i-1\n",
    "\n",
    "3- uni knows the past not the future\n",
    " bi-di have the past and the future (not used for real time usually. sometimes they are trained as bi but used as uni. but in that case half of the information that is required is missing)\n",
    "\n",
    " 4- next word prediction: uni. for real time application\n",
    " where would we use bi-dir; langugae translation. can better understand the context. when transalting we need to whol context. \n",
    " slower: it has more params, to train we have everything doubled and we have to reach from one side to the other. \n",
    " more memory and more time to train a bi-dir model. \n",
    " lstm, double sizes of gates. \n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
